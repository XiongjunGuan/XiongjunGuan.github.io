<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xiongjun Guan's Homepage</title>

    <meta name="author" content="Xiongjun Guan">
    <meta name="description" content="Hi, this is Xiongjun Guan, a student from Tsinghua University.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords"
        content="XiongjunGuan, Xiongjun Guan, xiongjunguan, xiongjun guan, Guanxiongjun,Guan Xiongjun, guanxiongjun, guan xiongjun, 关雄峻">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
    <table
        style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:60%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Xiongjun Guan</name>
                                    </p>
                                    <p>
                                        I am a fourth year Ph.D student in <a href="http://ivg.au.tsinghua.edu.cn/">
                                            i-VisionGroup </a> in
                                        the Department of Automation at Tsinghua University, advised by Prof. <a
                                            href="https://www.au.tsinghua.edu.cn/info/1078/3262.htm"> Jianjiang Feng
                                        </a> and Prof.
                                        <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm"> Jie Zhou
                                        </a>.
                                        <br>
                                        In 2021, I received my B.S. degree from the <a
                                            href="https://www.au.tsinghua.edu.cn/"> Department of
                                            Automation </a> and a minor from <a href="https://www.ad.tsinghua.edu.cn/">
                                            Academy of Arts and
                                            Design </a>, Tsinghua University.
                                    </p>
                                    <p>
                                        I have a broad interest in computer vision, pattern recognition and human
                                        computer interaction. At present,
                                        my research mainly
                                        focus on fingerprint recoginition and human computer interaction.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:gxj21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=O0E8IwYAAAAJ&hl=en"> Google
                                            Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://github.com/XiongjunGuan"> Github </a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:30%;max-width:30%">
                                    <img style="width:60%;max-width:60%" alt="profile photo" src="images/gxj.jpg">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <hr>
                                    <div class="news-slider">
                                        <p>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> We participated in <a
                                                    href="https://zeroqiaoba.github.io/MER2025-website/">MER 2025 @ ACM
                                                    MM </a>
                                                (multi-modal reliable emotion understanding based on VLMs).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> We participated in <a
                                                    href="https://egovis.github.io/cvpr25/">CVRR @ CVPR 2025 </a>
                                                (complex video reasoning & robustness evaluation).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> We achieved the <font color="red"><strong>2nd
                                                        place</strong>
                                                </font> on <a href="https://egovis.github.io/cvpr25/">Ego4D EgoSchema
                                                    Challenge @ CVPR 2025</a>
                                                (very
                                                long-form video understanding &
                                                question-answering).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on finger photo pose estimation is submitted to
                                                <a href="https://ijcb2025.ieee-biometrics.org/">IJCB 2025</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on fixed-length fingerprint descriptor is
                                                submitted
                                                to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on pose estimation for under-screen fingerprint
                                                sensor is submitted to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-04:</b> 1 paper on finger pose interaction is submitted to <a
                                                    href="https://uist.acm.org/2025/">UIST 2025</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-11:</b> 1 paper on partial fingerprint is accepted by <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-06:</b> 1 paper on latent fingerprint desctiptor is accepted by
                                                <a href="https://ijcb2024.ieee-biometrics.org/">IJCB 2024</a> as Poster.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-05:</b> 1 paper on fingerprint dense registration is accepted by
                                                <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2023-07:</b> 1 paper on fingerprint distortion rectification is
                                                accepted by
                                                <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2022-08:</b> 1 paper on fingerprint distortion rectification is
                                                accepted by
                                                <a href="https://ieee-biometrics.org/ijcb2022/#/">IJCB
                                                    2022</a> as Oral.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2021-09:</b> 1 paper on 3D fingerprint unfolding and visualization is
                                                accepted by
                                                <a href="https://ccbr99.cn/2021/">CCBR 2021</a> as Oral.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2021-09:</b> Joined <a
                                                    href="https://ivg.au.tsinghua.edu.cn/">Intelligent
                                                    Vision Group (i-VisionGroup)</a> as a Ph.D. candidate supervised by
                                                Prof. Jianjiang Feng & Prof. Jie Zhou.
                                            </li>
                                        </p>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <p>
                                        <heading>Research</heading>
                                    </p>
                                    <hr>
                                    <p>
                                        * indicates equal contribution
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <details>
                                        <summary style="cursor: pointer;">
                                            <subheading>Preprints</subheading>
                                        </summary>
                                        <br>
                                        <table
                                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                            <tbody>
                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-cvpr2025_VQA" id="return-cvpr2025_VQA">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/cvpr2025_VQA.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Four Eyes Are Better Than Two: Harnessing the
                                                            Collaborative
                                                            Potential
                                                            of Large Models via Differentiated Thinking and
                                                            Complementary Ensembles
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong> *,
                                                        Jun Xie *, Yingjian Zhu, Zhaoran Zhao, Xinming Wang, Feng Chen,
                                                        Zhepeng Wang
                                                        <br>
                                                        <strong>
                                                            <font color="red">2nd place</font> in CVPR25 competition
                                                        </strong>
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2505.16784>[Paper]</a>
                                                        <a href=https://egovis.github.io/cvpr25 />[Challenge]</a>
                                                        <a href=https://egoschema.github.io>[Info]</a>
                                                        <br>
                                                        <p> We present the runner-up solution for the Ego4D EgoSchema
                                                            Challenge at CVPR
                                                            2025
                                                            (Confirmed on May 20, 2025). Inspired by the success of
                                                            large models, we
                                                            evaluate and leverage leading accessible multimodal large
                                                            models and adapt
                                                            them
                                                            to video understanding tasks via few-shot learning and model
                                                            ensemble
                                                            strategies.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2025_DRACO" id="return-tifs2025_DRACO">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2025_DRACO.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Finger Pose Estimation for Under-screen Fingerprint
                                                            Sensor
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <a href=http://arxiv.org/abs/2505.02481>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DRACO>[Code]</a>
                                                        <br>
                                                        <p> We introduce a partial fingerprint pose estimation framework
                                                            that
                                                            leverages the collaborative potential of Dual-modal guidance
                                                            from Ridge
                                                            patches
                                                            And Capacitive images to Optimize the feature extraction,
                                                            fusion and
                                                            representation. Several simple but effective strategies and
                                                            mechanisms
                                                            are introduced, including knowledge transfer, MoE, and
                                                            decoupled probability
                                                            distribution, to enhance the network's capacity for
                                                            information mining and
                                                            interaction.
                                                        </p>
                                                    </td>
                                                </tr>


                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2025_FLARE" id="return-tifs2025_FLARE">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2025_FLARE.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Fixed-Length Dense Fingerprint Representation
                                                        </papertitle>
                                                        <br>
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <a href=https://arxiv.org/abs/2505.03597>[Paper]</a>
                                                        <a href=https://github.com/Yu-Yy/FLARE>[Code]</a>
                                                        <br>
                                                        <p> In this work, we propose a fixed-length dense descriptor of
                                                            fingerprints,
                                                            and
                                                            introduce FLARE—a fingerprint matching framework that
                                                            integrates the
                                                            Fixed-Length dense descriptor with pose-based Alignment and
                                                            Robust
                                                            Enhancement.
                                                            This fixed-length representation employs a three-dimensional
                                                            dense
                                                            descriptor to
                                                            effectively capture spatial relationships among fingerprint
                                                            ridge
                                                            structures,
                                                            enabling robust and locally discriminative representations.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-uist2025_BiFingerPose"
                                                            id="return-uist2025_BiFingerPose">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/uist2025_BiFingerPose.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> BiFingerPose: Bimodal Finger Pose Estimation for
                                                            Touch Device
                                                            Interaction
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>

                                                        <a>(<em>under review</em>)</a>
                                                        <br>
                                                        <p> In this paper, we estimate the 2D finger pose using a
                                                            multimodal network and
                                                            map
                                                            it to a standardized UV space, followed by nearly lossless
                                                            mapping to 3D
                                                            space
                                                            using simple polynomial functions. We further highlight the
                                                            applicability
                                                            and
                                                            appeal of finger pose in enhancing interactive experiences,
                                                            and develop
                                                            several
                                                            prototypes to demonstrate the potential for interaction.
                                                        </p>
                                                    </td>
                                                </tr>

                                            </tbody>
                                        </table>
                                    </details>
                                </td>
                            </tr>
                        </tbody>
                    </table>



                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <details>
                                        <summary style="cursor: pointer;">
                                            <subheading>Publications</subheading>
                                        </summary>
                                        <br>
                                        <table
                                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                            <tbody>
                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2024_partial" id="return-tifs2024_partial">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2024_partial.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Joint Identity Verification and Pose Alignment for
                                                            Partial Fingerprints
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2025
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2405.03959>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/JIPNet>[Code]</a>
                                                        <br>
                                                        <p> A novel framework for joint partial fingerprint identity
                                                            verification and
                                                            pose alignment of partial fingerprint pairs is proposed,
                                                            which utilizes a
                                                            multi-task
                                                            CNN-Transformer hybrid network and a pre-training task on
                                                            enhancement.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ijcb2024_descriptor"
                                                            id="return-ijcb2024_descriptor">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ijcb2024_descriptor.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Latent Fingerprint Matching via Dense Minutia
                                                            Descriptor
                                                        </papertitle>
                                                        <br>
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em> International Joint Conference on Biometrics
                                                            (<strong>IJCB</strong>)</em>, 2024
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2405.01199>[Paper]</a>
                                                        <a href=https://github.com/Yu-Yy/DMD>[Code]</a>
                                                        <br>
                                                        <p> Latent fingerprint matching is a daunting task, primarily
                                                            due to the poor
                                                            quality of latent fingerprints.
                                                            In this study, we propose a deep-learning based dense
                                                            minutia descriptor (DMD)
                                                            for latent fingerprint matching.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2024_registration"
                                                            id="return-tifs2024_registration">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2024_registration.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Phase-aggregated Dual-branch Network for Efficient
                                                            Fingerprint Dense Registration
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2024
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2404.17159>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/PDRNet-intro>[Code]</a>
                                                        <br>
                                                        <p> We propose a Phase-aggregated Dual-branch Registration
                                                            Network to combine the
                                                            strengths of
                                                            traditional fingerprint dense registration methods and deep
                                                            learning.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2023_rectification"
                                                            id="return-tifs2023_rectification">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2023_rectification.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Regression of Dense Distortion Field from a Single
                                                            Fingerprint Image
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2023
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17610">[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                                        <br>
                                                        <p> We proposed an end-to-end network to directly estimate a
                                                            dense distortion field
                                                            instead of
                                                            its low dimensional representation, from a single
                                                            fingerprint.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ijcb2022_rectification"
                                                            id="return-ijcb2022_rectification">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ijcb2022_rectification.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Direct Regression of Distortion Field from a Single
                                                            Fingerprint Image
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>International Joint Conference on Biometrics
                                                            (<strong>IJCB</strong>)</em>, 2022
                                                        <br>
                                                        <font color="red"><strong>Oral Presentation</strong></font>
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17148">[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                                        <a href="docs/IJCB2022.pptx">[Slide]</a>
                                                        <br>
                                                        <p> We proposed an end-to-end network to directly estimate a
                                                            dense distortion field
                                                            from a single
                                                            fingerprint instead of its low dimensional representation.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ccbr2021_3dfp" id="return-ccbr2021_3dfp">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ccbr2021_3dfp.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Pose-Specific 3D Fingerprint Unfolding</papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>Chinese Conference on Biometric Recognition
                                                            (<strong>CCBR</strong>)</em>, 2021
                                                        <br>
                                                        <font color="red"><strong>Oral Presentation</strong></font>
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17149">[Paper]</a>
                                                        <a href="docs/CCBR2021.pptx">[Slide]</a>
                                                        <br>
                                                        <p> We proposed a visualization and pose-specific unfolding
                                                            method for 3D
                                                            fingerprints, which can
                                                            improve the compatibility between 3D and 2D fingerprints in
                                                            recognition.
                                                        </p>
                                                    </td>
                                                </tr>

                                            </tbody>
                                        </table>
                                    </details>
                                </td>
                            </tr>
                        </tbody>
                    </table>





                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Honors and Awards</heading>
                                    <hr>
                                    <p>
                                        <li style="margin: 5px;"> Comprehensive Excellence Award, Tsinghua University,
                                            2022 & 2023 & 2024</li>
                                        <li style="margin: 5px;"> Silver Award for Social Practice of Ph.D Students,
                                            Tsinghua University,
                                            2023</li>
                                        <li style="margin: 5px;"> Excellent Graduates, Tsinghua University, 2021</li>
                                        <li style="margin: 5px;"> Outstanding Graduates, Tsinghua University (Dept. of
                                            Automation), 2021
                                        </li>
                                        <li style="margin: 5px;"> Academic Excellence Award, Tsinghua University, 2019 &
                                            2020</li>
                                        <li style="margin: 5px;"> 1st Prize in the 35th China Regional College Students
                                            Physics Competition,
                                            2018</li>
                                        <li style="margin: 5px;"> Gold Medal in the 33th Chinese Physics Olympiad (Top
                                            100 in the country),
                                            2016</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Teaching</heading>
                                    <hr>
                                    <p>
                                        <li style="margin: 5px;"> Teaching Assistant, Programming Fundamentals, 2024
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Interdisciplinary Research and
                                            Practice: Image Processing, 2023 Fall
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Basic of Information Theory, 2023
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Digital Image Processing, 2022
                                            Fall Semester</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <div id="img-cvpr2025_VQA" class="modal">
                        <div class="modal-content">
                            <a href="#return-cvpr2025_VQA" class="modal-background">
                                <img src="images/cvpr2025_VQA.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2025_DRACO" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2025_DRACO" class="modal-background">
                                <img src="images/tifs2025_DRACO.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2025_FLARE" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2025_FLARE" class="modal-background">
                                <img src="images/tifs2025_FLARE.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-uist2025_BiFingerPose" class="modal">
                        <div class="modal-content">
                            <a href="#return-uist2025_BiFingerPose" class="modal-background">
                                <img src="images/uist2025_BiFingerPose.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2024_partial" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_partial" class="modal-background">
                                <img src="images/tifs2024_partial.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-ijcb2024_descriptor" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2024_descriptor" class="modal-background">
                                <img src="images/ijcb2024_descriptor.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2024_registration" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_registration" class="modal-background">
                                <img src="images/tifs2024_registration.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-tifs2023_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2023_rectification" class="modal-background">
                                <img src="images/tifs2023_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ijcb2022_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2022_rectification" class="modal-background">
                                <img src="images/ijcb2022_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ccbr2021_3dfp" class="modal">
                        <div class="modal-content">
                            <a href="#return-ccbr2021_3dfp" class="modal-background">
                                <img src="images/ccbr2021_3dfp.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                </td>
            </tr>
    </table>

    <p>
        <center>
            <div id="clustrmaps-widget" style="width:5%">
                <script type="text/javascript" id="clstr_globe"
                    src="//clustrmaps.com/globe.js?d=_WZ_T7Ew-6-UikOWP9Jm-WvkAbk4Bh3KAv0GrUefMAA"></script>
            </div>
            <br>
            &copy; Xiongjun Guan | Last updated: May 28, 2025
        </center>
    </p>

</body>

</html>