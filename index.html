<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xiongjun Guan's Homepage</title>

    <meta name="author" content="Xiongjun Guan">
    <meta name="description" content="Hi, this is Xiongjun Guan, a student from Tsinghua University.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords"
        content="XiongjunGuan, Xiongjun Guan, xiongjunguan, xiongjun guan, Guanxiongjun,Guan Xiongjun, guanxiongjun, guan xiongjun, 关雄峻">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
    <table
        style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:60%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Xiongjun Guan</name>
                                    </p>
                                    <p>
                                        I am a fourth year Ph.D student in <a href="http://ivg.au.tsinghua.edu.cn/">
                                            i-VisionGroup </a> in
                                        the Department of Automation at Tsinghua University, advised by Prof. <a
                                            href="https://www.au.tsinghua.edu.cn/info/1078/3262.htm"> Jianjiang Feng
                                        </a> and Prof.
                                        <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm"> Jie Zhou
                                        </a>.
                                        <br>
                                        In 2021, I received my B.S. degree from the <a
                                            href="https://www.au.tsinghua.edu.cn/"> Department of
                                            Automation </a> and a minor from <a href="https://www.ad.tsinghua.edu.cn/">
                                            Academy of Arts and
                                            Design </a>, Tsinghua University.
                                    </p>
                                    <p>
                                        I have a broad interest in computer vision, pattern recognition and human
                                        computer interaction. At present,
                                        my research mainly
                                        focus on fingerprint recoginition and human computer interaction.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:gxj21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=O0E8IwYAAAAJ&hl=en"> Google
                                            Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://github.com/XiongjunGuan"> Github </a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:30%;max-width:30%">
                                    <img style="width:60%;max-width:60%" alt="profile photo" src="images/gxj.jpg">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                        <li style="margin: 5px;">
                                            <b>2025-011:</b> 1 paper on finger pose interaction is submitted to <a
                                                href="https://dl.acm.org/journal/imwut">IMWUT</a>.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>2025-011:</b> 1 paper on partial fingerprint is accepted by <a
                                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>2024-06:</b> 1 paper on latent fingerprint desctiptor is accepted by <a
                                                href="https://ijcb2024.ieee-biometrics.org/">IJCB 2024</a>.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>2024-05:</b> 1 paper on fingerprint dense registration is accepted by <a
                                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>2023-07:</b> 1 paper on fingerprint distortion rectification is accepted
                                            by <a
                                                href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                        </li>
                                        <li style="margin: 5px;">
                                            <b>2022-07:</b> 1 paper on fingerprint distortion rectification is accepted
                                            by <a href="https://ijcb2022.org/#/">IJCB 2022</a>.
                                        </li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <p>
                                        <heading>Research</heading>
                                    </p>
                                    <p>
                                        * indicates equal contribution
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <p>
                                        <subheading>&#9660 Preprints</subheading>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-tifs2024_partial" id="return-tifs2024_registration">
                                        <img style="width:100%;max-width:100%" src="images/arxiv2024_UniFiPose.PNG"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle> UniFingerPose: Multi Modal Universal Finger Pose Estimation for Touch
                                        Devices
                                    </papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a> ZhiyuPan </a>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
                                        Technologies (<strong>IMWUT</strong>)</em>, 2024
                                    <br>
                                    <a>(<em>under review</em>)</a>
                                    <br>
                                    <p> In this paper, a multi-modal universal finger pose estimation algorithm is
                                        proposed, which supports different types of sensors and can output 2D/3D finger
                                        pose simultaneously
                                        and precisely.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <p>
                                        <subheading>&#9660 Publications</subheading>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-tifs2024_partial" id="return-tifs2024_registration">
                                        <img style="width:100%;max-width:100%" src="images/tifs2024_partial.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle> Joint Identity Verification and Pose Alignment for Partial Fingerprints
                                    </papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a> ZhiyuPan </a>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>IEEE Transactions on Information Forensics and Security
                                        (<strong>T-IFS</strong>)</em>, 2024
                                    <br>
                                    <a href=https://arxiv.org/abs/2405.03959>(<em>under review</em>)</a>
                                    <a href=https://github.com/XiongjunGuan/JIPNet>[Code]</a>
                                    <br>
                                    <p> A novel framework for joint partial fingerprint identity verification and
                                        pose alignment of partial fingerprint pairs is proposed, which utilizes a
                                        multi-task
                                        CNN-Transformer hybrid network and a pre-training task on enhancement.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-ijcb2024_descriptor" id="return-tifs2024_registration">
                                        <img style="width:100%;max-width:100%" src="images/ijcb2024_descriptor.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle> Latent Fingerprint Matching via Dense Minutia Descriptor
                                    </papertitle>
                                    <br>
                                    <a> ZhiyuPan </a>,
                                    <a href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                        Yongjie Duan </a>,
                                    <strong> Xiongjun Guan </strong>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em> International Joint Conference on Biometrics (<strong>IJCB</strong>)</em>, 2024
                                    <br>
                                    <a href=https://arxiv.org/abs/2405.01199>[Paper]</a>
                                    <a href=https://github.com/Yu-Yy/DMD>[Code]</a>
                                    <br>
                                    <p> Latent fingerprint matching is a daunting task, primarily due to the poor
                                        quality of latent fingerprints.
                                        In this study, we propose a deep-learning based dense minutia descriptor (DMD)
                                        for latent fingerprint matching.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-tifs2024_registration" id="return-tifs2024_registration">
                                        <img style="width:100%;max-width:100%" src="images/tifs2024_registration.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle>Phase-aggregated Dual-branch Network for Efficient
                                        Fingerprint Dense Registration
                                    </papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>IEEE Transactions on Information Forensics and Security
                                        (<strong>T-IFS</strong>)</em>, 2024
                                    <br>
                                    <a href=https://arxiv.org/abs/2404.17159>[Paper]</a>
                                    <a href=https://github.com/XiongjunGuan/PDRNet-intro>[Code]</a>
                                    <br>
                                    <p> We propose a Phase-aggregated Dual-branch Registration Network to combine the
                                        strengths of
                                        traditional fingerprint dense registration methods and deep
                                        learning.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-tifs2023_rectification" id="return-tifs2023_rectification">
                                        <img style="width:100%;max-width:100%" src="images/tifs2023_rectification.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle>Regression of Dense Distortion Field from a Single Fingerprint Image
                                    </papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                        Yongjie Duan </a>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>IEEE Transactions on Information Forensics and Security
                                        (<strong>T-IFS</strong>)</em>, 2023
                                    <br>
                                    <a href="https://arxiv.org/abs/2404.17610">[Paper]</a>
                                    <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                    <br>
                                    <p> We proposed an end-to-end network to directly estimate a dense distortion field
                                        instead of
                                        its low dimensional representation, from a single fingerprint.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-ijcb2022_rectification" id="return-ijcb2022_rectification">
                                        <img style="width:100%;max-width:100%" src="images/ijcb2022_rectification.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle>Direct Regression of Distortion Field from a Single Fingerprint Image
                                    </papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                        Yongjie Duan </a>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>International Joint Conference on Biometrics (<strong>IJCB</strong>)</em>, 2022
                                    <br>
                                    <font color="red"><strong>Oral Presentation</strong></font>
                                    <br>
                                    <a href="https://arxiv.org/abs/2404.17148">[Paper]</a>
                                    <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                    <a href="docs/IJCB2022.pptx">[Slide]</a>
                                    <br>
                                    <p> We proposed an end-to-end network to directly estimate a dense distortion field
                                        from a single
                                        fingerprint instead of its low dimensional representation.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:30%;max-width:30%" align="center">
                                    <a href="#img-ccbr2021_3dfp" id="return-ccbr2021_3dfp">
                                        <img style="width:100%;max-width:100%" src="images/ccbr2021_3dfp.png"
                                            alt="dise">
                                    </a>
                                </td>
                                <td width="75%" valign="center">
                                    <papertitle>Pose-Specific 3D Fingerprint Unfolding</papertitle>
                                    <br>
                                    <strong> Xiongjun Guan </strong>,
                                    <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng </a>,
                                    <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                        Jie Zhou </a>

                                    <br>
                                    <em>Chinese Conference on Biometric Recognition (<strong>CCBR</strong>)</em>, 2021
                                    <br>
                                    <font color="red"><strong>Oral Presentation</strong></font>
                                    <br>
                                    <a href="https://arxiv.org/abs/2404.17149">[Paper]</a>
                                    <a href="docs/CCBR2021.pptx">[Slide]</a>
                                    <br>
                                    <p> We proposed a visualization and pose-specific unfolding method for 3D
                                        fingerprints, which can
                                        improve the compatibility between 3D and 2D fingerprints in recognition.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>



                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Honors and Awards</heading>
                                    <p>
                                        <li style="margin: 5px;"> Silver Award for Social Practice of Ph.D Students,
                                            Tsinghua University,
                                            2023</li>
                                        <li style="margin: 5px;"> Comprehensive Excellence Award, Tsinghua University,
                                            2022 & 2023</li>
                                        <li style="margin: 5px;"> Excellent Graduates, Tsinghua University, 2021</li>
                                        <li style="margin: 5px;"> Outstanding Graduates, Tsinghua University (Dept. of
                                            Automation), 2021
                                        </li>
                                        <li style="margin: 5px;"> Academic Excellence Award, Tsinghua University, 2019 &
                                            2020</li>
                                        <li style="margin: 5px;"> 1st Prize in the 35th China Regional College Students
                                            Physics Competition,
                                            2018</li>
                                        <li style="margin: 5px;"> Gold Medal in the 33th Chinese Physics Olympiad (Top
                                            100 in the country),
                                            2016</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Teaching</heading>
                                    <p>
                                        <li style="margin: 5px;"> Teaching Assistant, Programming Fundamentals, 2024
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Interdisciplinary Research and
                                            Practice: Image Processing, 2023 Fall
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Basic of Information Theory, 2023
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Digital Image Processing, 2022
                                            Fall Semester</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <div id="img-tifs2024_partial" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_partial" class="modal-background">
                                <img src="images/tifs2024_partial.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-ijcb2024_descriptor" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2024_descriptor" class="modal-background">
                                <img src="images/ijcb2024_descriptor.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2024_registration" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_registration" class="modal-background">
                                <img src="images/tifs2024_registration.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-tifs2023_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2023_rectification" class="modal-background">
                                <img src="images/tifs2023_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ijcb2022_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2022_rectification" class="modal-background">
                                <img src="images/ijcb2022_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ccbr2021_3dfp" class="modal">
                        <div class="modal-content">
                            <a href="#return-ccbr2021_3dfp" class="modal-background">
                                <img src="images/ccbr2021_3dfp.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                </td>
            </tr>
    </table>

    <p>
        <center>
            <div id="clustrmaps-widget" style="width:5%">
                <script type="text/javascript" id="clstr_globe"
                    src="//clustrmaps.com/globe.js?d=_WZ_T7Ew-6-UikOWP9Jm-WvkAbk4Bh3KAv0GrUefMAA"></script>
            </div>
            <br>
            &copy; Xiongjun Guan | Last updated: Nov. 27, 2024
        </center>
    </p>

</body>

</html>