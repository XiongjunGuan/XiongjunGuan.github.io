<!DOCTYPE HTML>

<style>
    #full {
        display: none;
    }
</style>

<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xiongjun Guan's Homepage</title>

    <meta name="author" content="Xiongjun Guan">
    <meta name="description" content="Hi, this is Xiongjun Guan, a student from Tsinghua University.">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords"
        content="XiongjunGuan, Xiongjun Guan, xiongjunguan, xiongjun guan, Guanxiongjun,Guan Xiongjun, guanxiongjun, guan xiongjun, 关雄峻">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
    <table
        style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:60%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Xiongjun Guan</name>
                                    </p>
                                    <p>
                                        I am a fourth year Ph.D student in <a href="http://ivg.au.tsinghua.edu.cn/">
                                            i-VisionGroup </a> in
                                        the Department of Automation at Tsinghua University, advised by Prof. <a
                                            href="https://www.au.tsinghua.edu.cn/info/1078/3262.htm"> Jianjiang Feng
                                        </a> and Prof.
                                        <a href="https://www.au.tsinghua.edu.cn/info/1152/3127.htm"> Jie Zhou
                                        </a>.
                                        <br>
                                        In 2021, I received my B.S. degree from the <a
                                            href="https://www.au.tsinghua.edu.cn/"> Department of
                                            Automation </a> and a minor from <a href="https://www.ad.tsinghua.edu.cn/">
                                            Academy of Arts and
                                            Design </a>, Tsinghua University.
                                    </p>
                                    <p>
                                        I have a broad interest in computer vision, pattern recognition and human
                                        computer interaction. At present,
                                        my research mainly
                                        focus on MLLMs, computer vision and Image Retrieval.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:gxj21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=O0E8IwYAAAAJ&hl=en"> Google
                                            Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://github.com/XiongjunGuan"> Github </a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:30%;max-width:30%">
                                    <img style="width:60%;max-width:60%" alt="profile photo" src="images/gxj.jpg">
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <hr>
                                    <div class="news-slider">
                                        <p>
                                            <li style="margin: 5px;">
                                                <b>2025-07:</b> 1 paper on fingerprint indexing is
                                                submitted
                                                to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8423754">T-BIOM</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-07:</b> We achieved the <font color="red"><strong>2nd
                                                        place</strong>
                                                </font> on <a href="https://zeroqiaoba.github.io/MER2025-website/">MER25
                                                    @ ACM MM
                                                </a>
                                                (multi-modal affective computing).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-07:</b> 1 paper on finger photo pose estimation is accepted by
                                                <a href="https://ijcb2025.ieee-biometrics.org/">IJCB 2025</a> as Oral.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-06:</b> 1 paper about our runner-up solution on <a
                                                    href="https://egovis.github.io/cvpr25/">Ego4D EgoSchema
                                                    Challenge @ CVPR 2025</a> is accepted by <a
                                                    href="https://egovis.github.io/cvpr25/">EgoVis @ CVPR 2025</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-06:</b> We achieved the <font color="red"><strong>2nd
                                                        place</strong>
                                                </font> on <a href="https://egovis.github.io/cvpr25/">CVRR @ CVPR 2025
                                                </a>
                                                (complex video reasoning & robustness evaluation).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> We achieved the <font color="red"><strong>2nd
                                                        place</strong>
                                                </font> on <a href="https://egovis.github.io/cvpr25/">Ego4D EgoSchema
                                                    Challenge @ CVPR 2025</a>
                                                (very
                                                long-form video understanding &
                                                question-answering).
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on fixed-length fingerprint descriptor is
                                                submitted
                                                to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on pose estimation for under-screen fingerprint
                                                sensor is submitted to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2025-05:</b> 1 paper on finger pose interaction is submitted to <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7755">T-MC</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-11:</b> 1 paper on partial fingerprint is accepted by <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-06:</b> 1 paper on latent fingerprint desctiptor is accepted by
                                                <a href="https://ijcb2024.ieee-biometrics.org/">IJCB 2024</a> as Poster.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2024-05:</b> 1 paper on fingerprint dense registration is accepted by
                                                <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2023-07:</b> 1 paper on fingerprint distortion rectification is
                                                accepted by
                                                <a
                                                    href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">T-IFS</a>.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2022-08:</b> 1 paper on fingerprint distortion rectification is
                                                accepted by
                                                <a href="https://ieee-biometrics.org/ijcb2022/#/">IJCB
                                                    2022</a> as Oral.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2021-09:</b> 1 paper on 3D fingerprint unfolding and visualization is
                                                accepted by
                                                <a href="https://ccbr99.cn/2021/">CCBR 2021</a> as Oral.
                                            </li>
                                            <li style="margin: 5px;">
                                                <b>2021-09:</b> Joined <a
                                                    href="https://ivg.au.tsinghua.edu.cn/">Intelligent
                                                    Vision Group (i-VisionGroup)</a> as a Ph.D. candidate supervised by
                                                Prof. Jianjiang Feng & Prof. Jie Zhou.
                                            </li>
                                        </p>
                                    </div>
                                </td>
                            </tr>
                        </tbody>
                    </table>


                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <p>
                                        <heading>Research</heading>
                                    </p>
                                    <hr>
                                    <p>
                                        * indicates equal contribution
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <details>
                                        <summary style="cursor: pointer;">
                                            <subheading>Preprints</subheading>
                                        </summary>
                                        <br>
                                        <table
                                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                            <tbody>
                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tbiom2025_dmd" id="return-tbiom2025_dmd">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tbiom2025_dmd.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Minutiae-Anchored Local Dense Representation for
                                                            Fingerprint Matching
                                                        </papertitle>
                                                        <br>
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>

                                                        <a href=https://arxiv.org/abs/2507.15297>[Paper]</a>
                                                        <a href=https://github.com/Yu-Yy/DMD>[Code]</a>
                                                        <br>
                                                        <p> we propose DMD, a minutiae-anchored local dense
                                                            representation which captures both fine-grained ridge
                                                            textures and discriminative minutiae features in a spatially
                                                            structured manner. Specifically, descriptors are extracted
                                                            from local patches centered and oriented on each detected
                                                            minutia, forming a
                                                            three-dimensional tensor, where two dimensions represent
                                                            spatial locations on the fingerprint plane and the third
                                                            encodes semantic features.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-mm2025_mer_track2" id="return-mm2025_mer_track2">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/mm2025_mer_track2.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> ZeroES: Zero-Shot Ensemble for Open-Vocabulary
                                                            Video Emotion
                                                            Recognition with Large Multimodal Models
                                                        </papertitle>
                                                        <br>
                                                        <strong>
                                                            <font color="red">2nd place</font>
                                                        </strong> in <em> International Conference on Multimedia
                                                            (<strong>MM</strong>) competition
                                                        </em>, 2025
                                                        <br>
                                                        Jun Xie *, Xiaohui Fan *, Zhenghao Zhang, Feng Chen, Hongzhu Yi,
                                                        Yinjian Zhu, <strong> Xiongjun Guan</strong>,
                                                        Xinming Wang, Yue Bi, Tao Zhang,
                                                        Zhepeng Wang
                                                        <br>
                                                        <a>[Paper]</a>
                                                        <a
                                                            href=https://zeroqiaoba.github.io/MER2025-website />[Challenge]</a>
                                                        <br>
                                                        <p> Emotion recognition has long grappled with the inherent
                                                            subjectivity and open-ended nature of human affect, where
                                                            predefined taxonomies falter against the vast, evolving
                                                            spectrum of emotional expression. We present ZeroES, a
                                                            Zero-Shot Ensemble framework that
                                                            redefines open-vocabulary video emotion recognition by
                                                            leveraging the raw capacity of large-scale vision-language
                                                            models (VLMs)
                                                            without task-specific optimization.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-mm2025_mer_track1" id="return-mm2025_mer_track1">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/mm2025_mer_track1.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> More Is Better: A MoE-Based Emotion Recognition
                                                            Framework with Human Preference Alignment
                                                        </papertitle>
                                                        <br>
                                                        Jun Xie *, Yingjian Zhu *, Feng Chen, Zhenghao Zhang, Xiaohui
                                                        Fan, Hongzhu Yi, Xinming Wang, Chen Yu, Yue Bi, Zhaoran Zhao,
                                                        <strong> Xiongjun Guan (corresponding author)</strong>,
                                                        Zhepeng Wang
                                                        <br>

                                                        <strong>
                                                            <font color="red">2nd place</font>
                                                        </strong> in <em> International Conference on Multimedia
                                                            (<strong>MM</strong>) competition
                                                        </em>, 2025
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2508.06036>[Paper]</a>
                                                        <a href=https://github.com/zhuyjan/MER2025-MRAC25>[Code]</a>
                                                        <a
                                                            href=https://zeroqiaoba.github.io/MER2025-website />[Challenge]</a>
                                                        <br>
                                                        <p> We propose a robust Mixture of Experts (MoE) emotion
                                                            recognition framework that integrates diverse modalities,
                                                            including Vision-Language Models and Action Unit
                                                            information. Using consensus-based pseudo-labeling and a
                                                            two-stage training process, we enhance label quality and
                                                            reduce bias through multi-expert voting and rule-based
                                                            re-ranking for human-aligned predictions.
                                                        </p>
                                                    </td>
                                                </tr>



                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-cvpr2025_cvrr" id="return-cvpr2025_cvrr">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/cvpr2025_cvrr.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Team of One: Cracking Complex Video QA with Model
                                                            Synergy
                                                        </papertitle>
                                                        <br>
                                                        Jun Xie *, Zhaoran Zhao *, <strong> Xiongjun Guan </strong>,
                                                        Yingjian Zhu, Hongzhu Yi, Xinming Wang,
                                                        Feng Chen,
                                                        Zhepeng Wang
                                                        <br>
                                                        <strong>
                                                            <font color="red">2nd place</font>
                                                        </strong> in <em>Computer Vision
                                                            and
                                                            Pattern Recognition (<strong>CVPR</strong>) competition
                                                        </em>, 2025
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2507.13820>[Paper]</a>
                                                        <a
                                                            href=https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite />[Challenge]</a>
                                                        <br>
                                                        <p> We propose a novel framework for open-ended video question
                                                            answering that enhances reasoning depth and robustness in
                                                            complex real-world scenarios, as benchmarked on the CVRR-ES
                                                            dataset.
                                                        </p>
                                                    </td>
                                                </tr>



                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2025_DRACO" id="return-tifs2025_DRACO">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2025_DRACO.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Finger Pose Estimation for Under-screen Fingerprint
                                                            Sensor
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <a href=http://arxiv.org/abs/2505.02481>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DRACO>[Code]</a>
                                                        <br>
                                                        <p> We introduce a partial fingerprint pose estimation framework
                                                            that
                                                            leverages the collaborative potential of Dual-modal guidance
                                                            from Ridge
                                                            patches
                                                            And Capacitive images to Optimize the feature extraction,
                                                            fusion and
                                                            representation. Several simple but effective strategies and
                                                            mechanisms
                                                            are introduced, including knowledge transfer, MoE, and
                                                            decoupled probability
                                                            distribution, to enhance the network's capacity for
                                                            information mining and
                                                            interaction.
                                                        </p>
                                                    </td>
                                                </tr>


                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2025_FLARE" id="return-tifs2025_FLARE">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2025_FLARE.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Fixed-Length Dense Fingerprint Representation
                                                        </papertitle>
                                                        <br>
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <a href=https://arxiv.org/abs/2505.03597>[Paper]</a>
                                                        <a href=https://github.com/Yu-Yy/FLARE>[Code]</a>
                                                        <br>
                                                        <p> In this work, we propose a fixed-length dense descriptor of
                                                            fingerprints,
                                                            and
                                                            introduce FLARE—a fingerprint matching framework that
                                                            integrates the
                                                            Fixed-Length dense descriptor with pose-based Alignment and
                                                            Robust
                                                            Enhancement.
                                                            This fixed-length representation employs a three-dimensional
                                                            dense
                                                            descriptor to
                                                            effectively capture spatial relationships among fingerprint
                                                            ridge
                                                            structures,
                                                            enabling robust and locally discriminative representations.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-uist2025_BiFingerPose"
                                                            id="return-uist2025_BiFingerPose">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/uist2025_BiFingerPose.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> BiFingerPose: Bimodal Finger Pose Estimation for
                                                            Touch Device
                                                            Interaction
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>

                                                        <a>(<em>under review</em>)</a>
                                                        <br>
                                                        <p> In this paper, we estimate the 2D finger pose using a
                                                            multimodal network and
                                                            map
                                                            it to a standardized UV space, followed by nearly lossless
                                                            mapping to 3D
                                                            space
                                                            using simple polynomial functions. We further highlight the
                                                            applicability
                                                            and
                                                            appeal of finger pose in enhancing interactive experiences,
                                                            and develop
                                                            several
                                                            prototypes to demonstrate the potential for interaction.
                                                        </p>
                                                    </td>
                                                </tr>

                                            </tbody>
                                        </table>
                                    </details>
                                </td>
                            </tr>
                        </tbody>
                    </table>



                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <details>
                                        <summary style="cursor: pointer;">
                                            <subheading>Publications</subheading>
                                        </summary>
                                        <br>
                                        <table
                                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                            <tbody>
                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ijcb2025_contactless"
                                                            id="return-ijcb2025_contactless">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ijcb2025_contactless.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Contactless Fingerprint Recognition Guided by 3D
                                                            Finger Pose
                                                        </papertitle>
                                                        <br>
                                                        Haoxiang Pei,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>International Joint Conference on Biometrics
                                                            (<strong>IJCB</strong>)</em>, 2025
                                                        <br>
                                                        <font color="red"><strong>Oral Presentation</strong></font>
                                                        <br>
                                                        <a>[Paper]</a>
                                                        <a>[Code]</a>
                                                        <br>
                                                        <p> We demonstrate
                                                            that 3D pose information of contactless fingerprints can be
                                                            utilized to enhance the robustness and performance of
                                                            existing recognition systems by guiding the acquisition
                                                            process
                                                            and constraining finger poses.
                                                        </p>
                                                    </td>
                                                </tr>


                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-cvpr2025_VQA" id="return-cvpr2025_VQA">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/cvpr2025_VQA.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Four Eyes Are Better Than Two: Harnessing the
                                                            Collaborative
                                                            Potential
                                                            of Large Models via Differentiated Thinking and
                                                            Complementary Ensembles
                                                        </papertitle>
                                                        <br>
                                                        Jun Xie *, <strong> Xiongjun Guan * (first
                                                            student author)</strong>,Yingjian Zhu, Zhaoran Zhao, Xinming
                                                        Wang,
                                                        Hongzhu Yi,
                                                        Feng Chen,
                                                        Zhepeng Wang
                                                        <br>

                                                        <strong>
                                                            <font color="red">2nd place</font>
                                                        </strong> in <em>Computer Vision
                                                            and
                                                            Pattern Recognition (<strong>CVPR</strong>) competition
                                                        </em>, 2025

                                                        <br>
                                                        <a href=https://arxiv.org/abs/2505.16784>[Paper]</a>
                                                        <a href=https://egoschema.github.io>[Challenge]</a>
                                                        <a
                                                            href=https://github.com/XiongjunGuan/EgoSchema-CVPR25>[Code]</a>
                                                        <br>
                                                        <p> We present the runner-up solution for the Ego4D EgoSchema
                                                            Challenge at CVPR
                                                            2025
                                                            (Confirmed on May 20, 2025). Inspired by the success of
                                                            large models, we
                                                            evaluate and leverage leading accessible multimodal large
                                                            models and adapt
                                                            them
                                                            to video understanding tasks via few-shot learning and model
                                                            ensemble
                                                            strategies.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2024_partial" id="return-tifs2024_partial">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2024_partial.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Joint Identity Verification and Pose Alignment for
                                                            Partial Fingerprints
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2025
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2405.03959>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/JIPNet>[Code]</a>
                                                        <br>
                                                        <p> A novel framework for joint partial fingerprint identity
                                                            verification and
                                                            pose alignment of partial fingerprint pairs is proposed,
                                                            which utilizes a
                                                            multi-task
                                                            CNN-Transformer hybrid network and a pre-training task on
                                                            enhancement.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ijcb2024_descriptor"
                                                            id="return-ijcb2024_descriptor">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ijcb2024_descriptor.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle> Latent Fingerprint Matching via Dense Minutia
                                                            Descriptor
                                                        </papertitle>
                                                        <br>
                                                        <a
                                                            href="https://scholar.google.com/citations?user=3tmfTMoAAAAJ">
                                                            ZhiyuPan </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em> International Joint Conference on Biometrics
                                                            (<strong>IJCB</strong>)</em>, 2024
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2405.01199>[Paper]</a>
                                                        <a href=https://github.com/Yu-Yy/DMD>[Code]</a>
                                                        <br>
                                                        <p> Latent fingerprint matching is a daunting task, primarily
                                                            due to the poor
                                                            quality of latent fingerprints.
                                                            In this study, we propose a deep-learning based dense
                                                            minutia descriptor (DMD)
                                                            for latent fingerprint matching.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2024_registration"
                                                            id="return-tifs2024_registration">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2024_registration.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Phase-aggregated Dual-branch Network for Efficient
                                                            Fingerprint Dense Registration
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2024
                                                        <br>
                                                        <a href=https://arxiv.org/abs/2404.17159>[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/PDRNet-intro>[Code]</a>
                                                        <br>
                                                        <p> We propose a Phase-aggregated Dual-branch Registration
                                                            Network to combine the
                                                            strengths of
                                                            traditional fingerprint dense registration methods and deep
                                                            learning.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-tifs2023_rectification"
                                                            id="return-tifs2023_rectification">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/tifs2023_rectification.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Regression of Dense Distortion Field from a Single
                                                            Fingerprint Image
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>IEEE Transactions on Information Forensics and Security
                                                            (<strong>T-IFS</strong>)</em>, 2023
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17610">[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                                        <br>
                                                        <p> We proposed an end-to-end network to directly estimate a
                                                            dense distortion field
                                                            instead of
                                                            its low dimensional representation, from a single
                                                            fingerprint.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ijcb2022_rectification"
                                                            id="return-ijcb2022_rectification">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ijcb2022_rectification.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Direct Regression of Distortion Field from a Single
                                                            Fingerprint Image
                                                        </papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=PqMp30oAAAAJ&hl=en&oi=ao">
                                                            Yongjie Duan </a>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>International Joint Conference on Biometrics
                                                            (<strong>IJCB</strong>)</em>, 2022
                                                        <br>
                                                        <font color="red"><strong>Oral Presentation</strong></font>
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17148">[Paper]</a>
                                                        <a href=https://github.com/XiongjunGuan/DDRNet-intro>[Code]</a>
                                                        <a href="docs/IJCB2022.pptx">[Slide]</a>
                                                        <br>
                                                        <p> We proposed an end-to-end network to directly estimate a
                                                            dense distortion field
                                                            from a single
                                                            fingerprint instead of its low dimensional representation.
                                                        </p>
                                                    </td>
                                                </tr>

                                                <tr>
                                                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                                                        <a href="#img-ccbr2021_3dfp" id="return-ccbr2021_3dfp">
                                                            <img style="width:100%;max-width:100%"
                                                                src="images/ccbr2021_3dfp.png" alt="dise">
                                                        </a>
                                                    </td>
                                                    <td width="75%" valign="center">
                                                        <papertitle>Pose-Specific 3D Fingerprint Unfolding</papertitle>
                                                        <br>
                                                        <strong> Xiongjun Guan </strong>,
                                                        <a href="http://ivg.au.tsinghua.edu.cn/~jfeng/"> Jianjiang Feng
                                                        </a>,
                                                        <a
                                                            href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">
                                                            Jie Zhou </a>

                                                        <br>
                                                        <em>Chinese Conference on Biometric Recognition
                                                            (<strong>CCBR</strong>)</em>, 2021
                                                        <br>
                                                        <font color="red"><strong>Oral Presentation</strong></font>
                                                        <br>
                                                        <a href="https://arxiv.org/abs/2404.17149">[Paper]</a>
                                                        <a href="docs/CCBR2021.pptx">[Slide]</a>
                                                        <br>
                                                        <p> We proposed a visualization and pose-specific unfolding
                                                            method for 3D
                                                            fingerprints, which can
                                                            improve the compatibility between 3D and 2D fingerprints in
                                                            recognition.
                                                        </p>
                                                    </td>
                                                </tr>

                                            </tbody>
                                        </table>
                                    </details>
                                </td>
                            </tr>
                        </tbody>
                    </table>





                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Honors and Awards</heading>
                                    <hr>
                                    <p>
                                        <li style="margin: 5px;"> Comprehensive Excellence Award, Tsinghua University,
                                            2022 & 2023 & 2024</li>
                                        <li style="margin: 5px;"> Silver Award for Social Practice of Ph.D Students,
                                            Tsinghua University,
                                            2023</li>
                                        <li style="margin: 5px;"> Excellent Graduates, Tsinghua University, 2021</li>
                                        <li style="margin: 5px;"> Outstanding Graduates, Tsinghua University (Dept. of
                                            Automation), 2021
                                        </li>
                                        <li style="margin: 5px;"> Academic Excellence Award, Tsinghua University, 2019 &
                                            2020</li>
                                        <li style="margin: 5px;"> 1st Prize in the 35th China Regional College Students
                                            Physics Competition,
                                            2018</li>
                                        <li style="margin: 5px;"> Gold Medal in the 33th Chinese Physics Olympiad (Top
                                            100 in the country),
                                            2016</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Teaching</heading>
                                    <hr>
                                    <p>
                                        <li style="margin: 5px;"> Teaching Assistant, Programming Fundamentals, 2024
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Interdisciplinary Research and
                                            Practice: Image Processing, 2023 Fall
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Basic of Information Theory, 2023
                                            Spring
                                            Semester</li>
                                        <li style="margin: 5px;"> Teaching Assistant, Digital Image Processing, 2022
                                            Fall Semester</li>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <div id="img-mm2025_mer_track2" class="modal">
                        <div class="modal-content">
                            <a href="#return-mm2025_mer_track2" class="modal-background">
                                <img src="images/mm2025_mer_track2.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-ijcb2025_contactless" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2025_contactless" class="modal-background">
                                <img src="images/ijcb2025_contactless.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tbiom2025_dmd" class="modal">
                        <div class="modal-content">
                            <a href="#return-tbiom2025_dmd" class="modal-background">
                                <img src="images/tbiom2025_dmd.png" alt="dise" style="width:100%">
                        </div>
                    </div>


                    <div id="img-mm2025_mer_track1" class="modal">
                        <div class="modal-content">
                            <a href="#return-mm2025_mer_track1" class="modal-background">
                                <img src="images/mm2025_mer_track1.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-cvpr2025_cvrr" class="modal">
                        <div class="modal-content">
                            <a href="#return-cvpr2025_cvrr" class="modal-background">
                                <img src="images/cvpr2025_cvrr.png" alt="dise" style="width:100%">
                        </div>
                    </div>


                    <div id="img-cvpr2025_VQA" class="modal">
                        <div class="modal-content">
                            <a href="#return-cvpr2025_VQA" class="modal-background">
                                <img src="images/cvpr2025_VQA.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2025_DRACO" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2025_DRACO" class="modal-background">
                                <img src="images/tifs2025_DRACO.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2025_FLARE" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2025_FLARE" class="modal-background">
                                <img src="images/tifs2025_FLARE.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-uist2025_BiFingerPose" class="modal">
                        <div class="modal-content">
                            <a href="#return-uist2025_BiFingerPose" class="modal-background">
                                <img src="images/uist2025_BiFingerPose.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2024_partial" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_partial" class="modal-background">
                                <img src="images/tifs2024_partial.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-ijcb2024_descriptor" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2024_descriptor" class="modal-background">
                                <img src="images/ijcb2024_descriptor.png" alt="dise" style="width:100%">
                        </div>
                    </div>

                    <div id="img-tifs2024_registration" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2024_registration" class="modal-background">
                                <img src="images/tifs2024_registration.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-tifs2023_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-tifs2023_rectification" class="modal-background">
                                <img src="images/tifs2023_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ijcb2022_rectification" class="modal">
                        <div class="modal-content">
                            <a href="#return-ijcb2022_rectification" class="modal-background">
                                <img src="images/ijcb2022_rectification.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                    <div id="img-ccbr2021_3dfp" class="modal">
                        <div class="modal-content">
                            <a href="#return-ccbr2021_3dfp" class="modal-background">
                                <img src="images/ccbr2021_3dfp.png" alt="dise" style="width:100%">
                        </div>
                    </div>
                </td>
            </tr>
    </table>

    <p>
        <center>
            <div id="clustrmaps-widget" style="width:5%">
                <script type="text/javascript" id="clstr_globe"
                    src="//clustrmaps.com/globe.js?d=_WZ_T7Ew-6-UikOWP9Jm-WvkAbk4Bh3KAv0GrUefMAA"></script>
            </div>
            <br>
            &copy; Xiongjun Guan | Last updated: Aug. 13, 2025
        </center>
    </p>

</body>

</html>